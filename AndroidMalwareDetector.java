package malwareDetection;

import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.functions;
import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineModel;
import org.apache.spark.ml.PipelineStage;
import org.apache.spark.ml.classification.RandomForestClassifier;
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator;
import org.apache.spark.ml.feature.StringIndexer;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.ml.feature.IndexToString;
import org.apache.spark.ml.feature.StringIndexerModel;
import org.apache.spark.ml.feature.StandardScaler;
import org.apache.spark.ml.feature.PCA;
import org.apache.spark.ml.feature.PCAModel;
import org.apache.spark.ml.linalg.Vector;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.io.OutputStream;
import java.util.*;
import java.util.stream.Collectors;

public class AndroidMalwareDetector {
    public static void main(String[] args) throws IOException {
        if (args.length < 2) {
            System.err.println("Usage: AndroidMalwareDetector <input path> <output path>");
            System.exit(1);
        }

        String inputPath = args[0];
        String outputPath = args[1];

        SparkSession spark = SparkSession.builder().appName("AndroidMalwareDetector").getOrCreate();

        // Load the KDD dataset
        Dataset<Row> data = spark.read().format("csv").option("header", "true").option("inferSchema", "true")
                .load(inputPath);

        // Index String Columns
        List<String> stringColumns = Arrays.asList("FlowID", "SourceIP", "DestinationIP", "Protocol", "Timestamp", "Label");
        List<StringIndexerModel> indexers = new ArrayList<>();

        for (String column : stringColumns) {
            StringIndexerModel indexer = new StringIndexer()
                    .setInputCol(column)
                    .setOutputCol(column.toLowerCase() + "_index")
                    .setHandleInvalid("skip")
                    .fit(data);
            indexers.add(indexer);
            data = indexer.transform(data);
            data = data.drop(column);
        }

        // Collect numeric columns
        List<String> numericColumns = Arrays.stream(data.dtypes())
                .filter(tuple -> !tuple._2.equals("StringType"))
                .map(tuple -> tuple._1)
                .collect(Collectors.toList());

        // Remove label_index column from numeric columns
        numericColumns.remove("label_index");

        // Fill null values with 0.0
        for (String column : numericColumns) {
            data = data.withColumn(column, functions.when(data.col(column).isNull(), 0.0).otherwise(data.col(column)));
        }

        // Compute correlation coefficients
        Map<String, Double> featureCorrelations = new HashMap<>();
        for (String feature : numericColumns) {
            double correlation = data.stat().corr("label_index", feature);
            featureCorrelations.put(feature, correlation);
        }

        // Select the top 30 features based on correlation
        List<String> topFeatures = featureCorrelations.entrySet()
                .stream()
                .sorted(Map.Entry.<String, Double>comparingByValue().reversed())
                .limit(30)
                .map(Map.Entry::getKey)
                .collect(Collectors.toList());

        // Assemble top features into a single vector
        VectorAssembler assembler = new VectorAssembler()
                .setInputCols(topFeatures.toArray(new String[0]))
                .setOutputCol("features");

        // Normalize the features
        StandardScaler scaler = new StandardScaler()
                .setInputCol("features")
                .setOutputCol("scaledFeatures")
                .setWithStd(true)
                .setWithMean(true);

        // Apply PCA
        PCA pcaNonScaled = new PCA()
                .setInputCol("features")
                .setOutputCol("pcaFeatures")
                .setK(30); // Number of principal components to keep

        // Apply PCA
        PCA pcaScaled = new PCA()
                .setInputCol("scaledFeatures")
                .setOutputCol("pcaFeaturesScaled")
                .setK(30); // Number of principal components to keep

        final int maxBins = 70;
        final int numTrees = 10;
        final int maxDepth = 20;

        // Define Random Forest model with reduced parameters
        RandomForestClassifier rf = new RandomForestClassifier()
                .setLabelCol("label_index")
                .setFeaturesCol("features")
                .setMaxBins(maxBins) // Setting maxBins to be greater than the number of categories in the categorical feature
                .setNumTrees(numTrees) // Reduced number of trees
                .setMaxDepth(maxDepth); // Reduced max depth

        RandomForestClassifier rfScaled = new RandomForestClassifier()
                .setLabelCol("label_index")
                .setFeaturesCol("scaledFeatures")
                .setMaxBins(maxBins)
                .setNumTrees(numTrees)
                .setMaxDepth(maxDepth);

        RandomForestClassifier rfPca = new RandomForestClassifier()
                .setLabelCol("label_index")
                .setFeaturesCol("pcaFeatures")
                .setMaxBins(maxBins)
                .setNumTrees(numTrees)
                .setMaxDepth(maxDepth);

        RandomForestClassifier rfPcaScaled = new RandomForestClassifier()
                .setLabelCol("label_index")
                .setFeaturesCol("pcaFeaturesScaled")
                .setMaxBins(maxBins)
                .setNumTrees(numTrees)
                .setMaxDepth(maxDepth);

        // Convert indexed labels back to original labels
        IndexToString labelConverter = new IndexToString()
                .setInputCol("prediction")
                .setOutputCol("predictedLabel")
                .setLabels(indexers.get(indexers.size() - 1).labels());

        // Create pipelines
        Pipeline pipelineNoNormNoPca = new Pipeline().setStages(new PipelineStage[]{assembler, rf, labelConverter});
        Pipeline pipelineNormNoPca = new Pipeline().setStages(new PipelineStage[]{assembler, scaler, rfScaled, labelConverter});
        Pipeline pipelineNoNormPca = new Pipeline().setStages(new PipelineStage[]{assembler, pcaNonScaled, rfPca, labelConverter});
        Pipeline pipelineNormPca = new Pipeline().setStages(new PipelineStage[]{assembler, scaler, pcaScaled, rfPcaScaled, labelConverter});

        // Lists to store accuracies and run times
        Map<String, List<Double>> accuraciesTrain = new HashMap<>();
        Map<String, List<Double>> accuraciesTest = new HashMap<>();
        Map<String, List<Long>> runTimes = new HashMap<>();

        List<String> pipelineNames = Arrays.asList("NoNormNoPca", "NormNoPca", "NoNormPca", "NormPca");
        for (String name : pipelineNames) {
            accuraciesTrain.put(name, new ArrayList<>());
            accuraciesTest.put(name, new ArrayList<>());
            runTimes.put(name, new ArrayList<>());
        }

        // Initialize HDFS configuration and FileSystem
        Configuration hadoopConf = new Configuration();
        FileSystem fs = FileSystem.get(hadoopConf);

        // Write feature correlations and PCA explained variance to a file
        String featureStatsFilePath = outputPath + "/feature_stats.txt";
        try (OutputStream out = fs.create(new Path(featureStatsFilePath))) {
            out.write("Feature Correlations:\n".getBytes());
            for (Map.Entry<String, Double> entry : featureCorrelations.entrySet()) {
                out.write((entry.getKey() + ": " + entry.getValue() + "\n").getBytes());
            }
            
        } catch (Exception e) {
            e.printStackTrace();
        }

        // Run the Random Forest model with different configurations
        for (int i = 1; i <= 5; i++) { // Run each pipeline 5 times
            long seed = System.currentTimeMillis();
            Dataset<Row>[] splits = data.randomSplit(new double[]{0.7, 0.3}, seed);
            Dataset<Row> trainData = splits[0];
            Dataset<Row> testData = splits[1];

            for (String name : pipelineNames) {
                Pipeline pipeline;
                switch (name) {
                    case "NoNormNoPca":
                        pipeline = pipelineNoNormNoPca;
                        break;
                    case "NormNoPca":
                        pipeline = pipelineNormNoPca;
                        break;
                    case "NoNormPca":
                        pipeline = pipelineNoNormPca;
                        break;
                    case "NormPca":
                        pipeline = pipelineNormPca;
                        break;
                    default:
                        throw new IllegalArgumentException("Invalid pipeline name: " + name);
                }

                long startTime = System.currentTimeMillis();

                // Train model
                PipelineModel model = pipeline.fit(trainData);

                // Make predictions
                Dataset<Row> predictionsTrain = model.transform(trainData);
                Dataset<Row> predictionsTest = model.transform(testData);

                // Evaluate the model
                MulticlassClassificationEvaluator evaluatorTrain = new MulticlassClassificationEvaluator()
                        .setLabelCol("label_index")
                        .setPredictionCol("prediction")
                        .setMetricName("accuracy");
                double accuracyTrain = evaluatorTrain.evaluate(predictionsTrain);
                accuraciesTrain.get(name).add(accuracyTrain);

                MulticlassClassificationEvaluator evaluatorTest = new MulticlassClassificationEvaluator()
                        .setLabelCol("label_index")
                        .setPredictionCol("prediction")
                        .setMetricName("accuracy");
                double accuracyTest = evaluatorTest.evaluate(predictionsTest);
                accuraciesTest.get(name).add(accuracyTest);

                long endTime = System.currentTimeMillis();
                long runTime = endTime - startTime;
                runTimes.get(name).add(runTime);

                // Write the accuracy to HDFS
                String outputFilePath = outputPath + "/random_forest_accuracy_run_" + i + "_" + name + ".txt";
                try (OutputStream out = fs.create(new Path(outputFilePath))) {
                    out.write(("Training Accuracy: " + accuracyTrain + "\n").getBytes());
                    out.write(("Test Accuracy: " + accuracyTest + "\n").getBytes());
                    out.write(("Run time (ms): " + runTime + "\n").getBytes());
                } catch (Exception e) {
                    e.printStackTrace();
                }
            }
        }

        // Write statistics to HDFS for each pipeline configuration
        for (String name : pipelineNames) {
            List<Double> accTrain = accuraciesTrain.get(name);
            List<Double> accTest = accuraciesTest.get(name);
            List<Long> runtimes = runTimes.get(name);

            double maxAccTrain = accTrain.stream().mapToDouble(v -> v).max().orElse(0.0);
            double minAccTrain = accTrain.stream().mapToDouble(v -> v).min().orElse(0.0);
            double avgAccTrain = accTrain.stream().mapToDouble(v -> v).average().orElse(0.0);
            double stdDevAccTrain = Math.sqrt(accTrain.stream().mapToDouble(v -> Math.pow(v - avgAccTrain, 2)).sum() / accTrain.size());

            double maxAccTest = accTest.stream().mapToDouble(v -> v).max().orElse(0.0);
            double minAccTest = accTest.stream().mapToDouble(v -> v).min().orElse(0.0);
            double avgAccTest = accTest.stream().mapToDouble(v -> v).average().orElse(0.0);
            double stdDevAccTest = Math.sqrt(accTest.stream().mapToDouble(v -> Math.pow(v - avgAccTest, 2)).sum() / accTest.size());

            String statsOutputFilePath = outputPath + "/random_forest_statistics_" + name + ".txt";
            try (OutputStream out = fs.create(new Path(statsOutputFilePath))) {
                out.write(("Number of features: " + topFeatures.size() + "\n").getBytes());
                out.write(("Training Min Accuracy: " + minAccTrain + "\n").getBytes());
                out.write(("Training Max Accuracy: " + maxAccTrain + "\n").getBytes());
                out.write(("Training Average Accuracy: " + avgAccTrain + "\n").getBytes());
                out.write(("Training Standard Deviation of Accuracy: " + stdDevAccTrain + "\n").getBytes());
                out.write(("Test Min Accuracy: " + minAccTest + "\n").getBytes());
                out.write(("Test Max Accuracy: " + maxAccTest + "\n").getBytes());
                out.write(("Test Average Accuracy: " + avgAccTest + "\n").getBytes());
                out.write(("Test Standard Deviation of Accuracy: " + stdDevAccTest + "\n").getBytes());
                out.write(("Run Times (ms): " + runtimes + "\n").getBytes());
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        spark.stop();
    }
}
